{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c44ca30f",
      "metadata": {
        "id": "c44ca30f"
      },
      "source": [
        "# CognitiveLab Research Internship Assignment\n",
        "\n",
        "## Synthetic Data Generation and LLM Fine-tuning\n",
        "\n",
        "### Overview\n",
        "In this assignment for [CognitiveLab](https://cognitivelab.in/) Research Internship, you will:\n",
        "1. Create a synthetic dataset for a use case of your choice\n",
        "2. Fine-tune a small LLM using this dataset\n",
        "3. Evaluate the model performance before and after fine-tuning\n",
        "\n",
        "### What is Synthetic Data?\n",
        "Synthetic data refers to artificially generated data rather than data collected from real-world events. In the context of this assignment:\n",
        "- Data generated using AI/ML algorithms\n",
        "- Data transformed from existing datasets (e.g., translations)\n",
        "- Data created by prompting LLMs to generate samples\n",
        "\n",
        "### Task Description\n",
        "You're expected to:\n",
        "1. Choose an interesting use case (suggestions below)\n",
        "2. Generate a high-quality synthetic dataset\n",
        "3. Fine-tune a small LLM (1-2B parameters)\n",
        "4. Thoroughly evaluate the results\n",
        "5. Document your approach and findings\n",
        "\n",
        "### Potential Use Cases\n",
        "- **Multilingual Translation**: Generate translation pairs for low-resource languages\n",
        "- **Reasoning Tasks**: Create logical or mathematical reasoning problems\n",
        "- **Vision-Language OCR**: Generate text extraction examples from images\n",
        "- **Domain-Specific Q&A**: Create question-answer pairs for specialized domains\n",
        "- **Code Generation**: Generate code examples for specific programming tasks\n",
        "\n",
        "### Requirements\n",
        "- The assignment must run on Google Colab with a single click\n",
        "- Your synthetic dataset must be uploaded to Hugging Face Datasets\n",
        "- Use small language models (eg Llama 3.2 1B and Qwen 3 0.6B / 1.7B) that can run on T4 GPUs\n",
        "- Include comprehensive documentation of your approach\n",
        "\n",
        "Good luck! We're looking for creative approaches to this problem - surprise us with your solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b92f5b1",
      "metadata": {
        "id": "5b92f5b1"
      },
      "source": [
        "## 1. What is you Idea and use case you are trying to solve?\n",
        "\n",
        "Give up a tldr of your idea and the use case you are trying to solve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "781ea0f2",
      "metadata": {
        "id": "781ea0f2"
      },
      "source": [
        "## 2. Environment Setup\n",
        "\n",
        "In this section, we'll install all the necessary dependencies for our project. This includes libraries for:\n",
        "- Data processing and manipulation\n",
        "- LLM access and fine-tuning\n",
        "- Evaluation metrics\n",
        "- Hugging Face integration for dataset upload and model download\n",
        "\n",
        "Run the cell below to set up your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33819aa6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33819aa6",
        "outputId": "3451e735-84b4-4505-9628-20f4483cfa5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m961.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Transformers version: 4.51.3\n",
            "Datasets version: 3.6.0\n",
            "PEFT version: 0.15.2\n",
            "Fri May 23 07:14:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Install necessary dependencies\n",
        "!pip install -q transformers datasets evaluate peft bitsandbytes accelerate\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q trl\n",
        "!pip install -q nltk rouge-score sacrebleu\n",
        "\n",
        "# Optional: For specific use cases\n",
        "# !pip install -q sentencepiece tokenizers\n",
        "# !pip install -q gradio # For demo creation\n",
        "\n",
        "# Login to Hugging Face (you'll need a token)\n",
        "from huggingface_hub import login\n",
        "# Uncomment the line below and add your token when ready to upload datasets\n",
        "# login()\n",
        "\n",
        "# Verify installations\n",
        "import transformers\n",
        "import datasets\n",
        "import peft\n",
        "\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "print(f\"PEFT version: {peft.__version__}\")\n",
        "\n",
        "# Check available GPU\n",
        "!nvidia-smi\n",
        "# ideally a T4 or A100 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9b07cc",
      "metadata": {
        "id": "ff9b07cc"
      },
      "source": [
        "## 3. Synthetic Data Generation\n",
        "\n",
        "In this section, we'll generate a synthetic dataset for our selected use case. The process involves:\n",
        "\n",
        "1. Defining the data structure and schema\n",
        "2. Setting up data generation techniques (LLM prompting, rules-based generation, etc.)\n",
        "3. Creating the dataset\n",
        "4. Validating data quality\n",
        "5. Uploading to Hugging Face Datasets\n",
        "\n",
        "Good examples of synthetic data generation include:\n",
        "- Using LLMs to generate text samples based on specific prompts\n",
        "- Using existing datasets to create variations (e.g., translations, paraphrasing)\n",
        "- Using rules-based systems to generate structured data (e.g., JSON, CSV)\n",
        "- Using data augmentation techniques to create variations of existing data\n",
        "- Using generative models to create new data points based on existing distributions\n",
        "\n",
        "ps: you can always start with a existing dataset and augment it with synthetically.\n",
        "\n",
        "Some libraries you can use for data generation:\n",
        "- https://github.com/meta-llama/synthetic-data-kit\n",
        "- https://github.com/argilla-io/distilabel\n",
        "- https://github.com/argilla-io/synthetic-data-generator\n",
        "\n",
        "For llms you can use local llm , use free apis from [groq](https://groq.com/) anything else you can find.\n",
        "\n",
        "Choose one of the example approaches below or create your own. Remember to document your methodology.\n",
        "\n",
        "### the dataset generated must be uploaded to hugging face datasets\n",
        "refrence : https://huggingface.co/docs/datasets/upload_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24076bc3",
      "metadata": {
        "id": "24076bc3"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Create a synthetic dataset\n",
        "\n",
        "\n",
        "# Optional: Upload to Hugging Face Datasets\n",
        "def upload_dataset_to_hf():\n",
        "    # Uncomment when ready to upload\n",
        "    # dataset_dict = DatasetDict({\n",
        "    #     \"train\": train_test[\"train\"],\n",
        "    #     \"test\": train_test[\"test\"]\n",
        "    # })\n",
        "    # dataset_dict.push_to_hub(\n",
        "    #     f\"your-username/{PROJECT_CONFIG['dataset_name']}\",\n",
        "    #     private=False\n",
        "    # )\n",
        "    # print(f\"Dataset uploaded to Hugging Face: your-username/{PROJECT_CONFIG['dataset_name']}\")\n",
        "    print(\n",
        "        \"Dataset upload code is ready but commented out. Uncomment when ready to upload.\"\n",
        "    )\n",
        "\n",
        "\n",
        "upload_dataset_to_hf()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee305c4",
      "metadata": {
        "id": "eee305c4"
      },
      "source": [
        "## 4. Model Fine-tuning\n",
        "\n",
        "Now that we have our synthetic dataset, let's fine-tune a small LLM using PEFT/LoRA techniques. This approach allows us to efficiently adapt the pre-trained model to our specific task without excessive computational requirements.\n",
        "\n",
        "We'll:\n",
        "1. Load the pre-trained model\n",
        "2. Prepare the dataset in the correct format\n",
        "3. Configure LoRA adapters\n",
        "4. Fine-tune the model\n",
        "5. Save the fine-tuned model\n",
        "\n",
        "This section uses Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA) to update only a small number of parameters, making it suitable for running on Colab's T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5ad904",
      "metadata": {
        "id": "9f5ad904"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3f1df861",
      "metadata": {
        "id": "3f1df861"
      },
      "source": [
        "## 5. Model Evaluation\n",
        "\n",
        "Now that we have fine-tuned our model, let's evaluate its performance by comparing it with the base model. We'll assess how well our synthetic data helped improve the model's abilities on our target task.\n",
        "\n",
        "We'll:\n",
        "1. Load both the base and fine-tuned models\n",
        "2. Define appropriate evaluation metrics\n",
        "3. Perform inference on test examples\n",
        "4. Compare and analyze the results\n",
        "5. Visualize performance differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0e14f0",
      "metadata": {
        "id": "3f0e14f0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "84ef42d8",
      "metadata": {
        "id": "84ef42d8"
      },
      "source": [
        "## 6. Final Thoughts and Project Analysis\n",
        "\n",
        "In this section, reflect on your approach, findings, and potential improvements.\n",
        "\n",
        "### Project Summary\n",
        "Provide a brief overview of what you did:\n",
        "- What use case did you choose and why?\n",
        "- How did you generate the synthetic dataset?\n",
        "- Which model did you fine-tune and what techniques did you use?\n",
        "- What were the main evaluation metrics and results?\n",
        "\n",
        "### Analysis of Results\n",
        "- Did fine-tuning improve performance? If so, how much?\n",
        "- Were there specific types of examples where improvement was more noticeable?\n",
        "- What limitations did you observe in your approach?\n",
        "\n",
        "### Improvement Ideas\n",
        "- How could you enhance the quality of the synthetic dataset?\n",
        "- What other fine-tuning approaches might work better?\n",
        "- If you had more computational resources, what would you do differently?\n",
        "\n",
        "### Learning Outcomes\n",
        "- What insights did you gain about synthetic data generation?\n",
        "- What did you learn about fine-tuning LLMs?\n",
        "- What surprised you during this project?\n",
        "\n",
        "Remember to support your analysis with specific examples and metrics from your evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398ed131",
      "metadata": {
        "id": "398ed131"
      },
      "source": [
        "## 7. References\n",
        "\n",
        "List any resources, papers, tutorials, or tools that you found helpful for this assignment:\n",
        "\n",
        "1. [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft/index)\n",
        "2. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "3. [Parameter-Efficient Fine-Tuning Methods](https://huggingface.co/blog/peft)\n",
        "4. [Synthetic Data Generation Techniques](https://arxiv.org/abs/2111.02739)\n",
        "5. [Evaluating Large Language Models](https://arxiv.org/abs/2307.03109)\n",
        "\n",
        "*This notebook was created as part of the CognitiveLab Research Internship assignment.*"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}